{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef05b88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7fc080546090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "from models.RLModelWrapper import TDWrapper\n",
    "from models.Builders import build_fc_dqnn\n",
    "from loggers import MetricLogger\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import FrameStack, GrayScaleObservation\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "# from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "# import gym_super_mario_bros\n",
    "\n",
    "gym.envs.registration.registry.keys()\n",
    "\n",
    "# TODO: Make into reproduciblity function call \n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# This is going to be necessary for perfect reproducibility\n",
    "# print(os.environ[\"CUBLAS_WORKSPACE_CONFIG\"])\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6307bac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495104.7823834813\n"
     ]
    }
   ],
   "source": [
    "def decay_to_halflife(decay: float) -> float: \n",
    "    # p * decay^halflife = .5p \n",
    "    return np.log(.5) / np.log(decay)\n",
    "\n",
    "def halflife_to_decay(halflife: float) -> float: \n",
    "    # p * decay^halflife = .5p\n",
    "    return np.power(.5, 1.0 / halflife)\n",
    "\n",
    "print(decay_to_halflife(.9999986))\n",
    "halflife_to_decay(1000)\n",
    "\n",
    "def plot_means(values_t: torch.tensor, meanlen: int):\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(values_t) >= meanlen:\n",
    "        means = values_t.unfold(0, meanlen, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(meanlen - 1), means))\n",
    "        plt.plot(means.numpy())\n",
    "    \n",
    "def plot_values(values, show_result=False, stop_episode=None):\n",
    "    plt.figure(1)\n",
    "    values_t = torch.tensor(values, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values_t.numpy())\n",
    "    plot_means(values_t, 100)\n",
    "    \n",
    "    if stop_episode is not None: \n",
    "        plt.axvline(stop_episode, c = 'r')\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cca5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,),\n",
      " 1.0,\n",
      " False,\n",
      " {}\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"CartPole-v1\", apply_api_compatibility=False)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")\n",
    "print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701b03d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get model checkpoint. Starting from scratch\n"
     ]
    }
   ],
   "source": [
    "# Model Definition \n",
    "model_info = dict(\n",
    "    state_dim = 4,\n",
    "    action_dim = env.action_space.n,\n",
    "    save_dir = Path(\"checkpoints\") / \"CartPolev1\",\n",
    "    batch_size = 128,\n",
    "    hidden_layers = 1, \n",
    "    layer_size = 128,\n",
    "    learn_every = 1,\n",
    "    learning_rate = .0001, \n",
    "    exploration_rate = .9,\n",
    "    exploration_rate_decay = .999,\n",
    "    exploration_rate_min = 0.0,\n",
    "    discount = .99,\n",
    "    replay_sample_strategy = \"priority\",\n",
    "    memory_capacity = 10000,\n",
    "    burnin = 128,\n",
    "    sync_every = 1,\n",
    "    sync_strategy = \"soft\", \n",
    "    sync_rate = .005\n",
    ")\n",
    "\n",
    "# increase learning rate, decrease exploration_rate_decay\n",
    "# increase memory size\n",
    "# shrink network -- slow training could be due to network size (which is certainly too large)\n",
    "\n",
    "net = build_fc_dqnn(model_info[\"state_dim\"], model_info[\"action_dim\"], model_info)\n",
    "model = TDWrapper(net, model_info)\n",
    "try: \n",
    "    model.get_checkpoint()\n",
    "except: \n",
    "    print(\"Failed to get model checkpoint. Starting from scratch\")\n",
    "    \n",
    "logger = MetricLogger(model_info[\"save_dir\"], rolling_avg_len = 10, record_every = 10, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea00f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 21 - Epsilon 0.8812878083682346 - Mean Reward 21.0 - Mean Length 21.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.63 - Time 2023-05-04T14:46:49\n",
      "Episode 100 - Step 10045 - Epsilon 3.8866169417937074e-05 - Mean Reward 265.3 - Mean Length 265.3 - Mean Loss 3.856 - Mean Q Value 31.833 - Time Delta 10.237 - Time 2023-05-04T14:47:30\n",
      "Episode 200 - Step 32739 - Epsilon 5.3550254585970225e-15 - Mean Reward 160.3 - Mean Length 160.3 - Mean Loss 4.991 - Mean Q Value 61.299 - Time Delta 5.449 - Time 2023-05-04T14:48:46\n",
      "Episode 300 - Step 74562 - Epsilon 3.59894835973496e-33 - Mean Reward 500.0 - Mean Length 500.0 - Mean Loss 20.301 - Mean Q Value 68.098 - Time Delta 16.528 - Time 2023-05-04T14:51:05\n",
      "Episode 400 - Step 123927 - Epsilon 1.2779147730968398e-54 - Mean Reward 491.8 - Mean Length 491.8 - Mean Loss 13.978 - Mean Q Value 61.729 - Time Delta 16.428 - Time 2023-05-04T14:53:50\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 500\n",
    "start_ep = model.episodes\n",
    "end_ep = start_ep + n_episodes\n",
    "end_burnin = None\n",
    "for e in range(start_ep, end_ep):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = model.act(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        model.cache(state = state, next_state = next_state, \n",
    "                    action = action, reward = reward, done = done)\n",
    "        q, loss = model.learn()\n",
    "        if model.curr_step > model.burnin and end_burnin is None: \n",
    "            end_burnin = e\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward = reward, loss = loss, q = q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done: #or info[\"flag_get\"]:\n",
    "            stats = model.end_episode()\n",
    "            break\n",
    "\n",
    "    logger.log_episode(stats)\n",
    "#     plot_values(logger.ep_rewards, stop_episode = end_burnin)\n",
    "\n",
    "logger.record(stats, do_print = True)\n",
    "# plot_values(logger.ep_rewards, show_result = True, stop_episode = end_burnin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71129f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: \n",
    "    render_env = gym.make(\"CartPole-v1\", render_mode=\"human\", apply_api_compatibility=False)\n",
    "\n",
    "    state = render_env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = model.act(state)#, exploit_only = True)\n",
    "        next_state, reward, terminated, truncated, info = render_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "        # Check if end of game\n",
    "        if done: #or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    render_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201dbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build some tests.\n",
    "# This was to see if memory worked the same in my module and the working one. \n",
    "# Then to see if the td computations were equivalent. \n",
    "\n",
    "transitions = []\n",
    "state, _ = env.reset()\n",
    "for _ in range(20):\n",
    "    action = model.act(state)\n",
    "    next_state, reward, done, trunc, info = env.step(action)\n",
    "    transitions.append((state, next_state, action, reward, done))\n",
    "    state = next_state\n",
    "    # Check if end of game\n",
    "    if done: #or info[\"flag_get\"]:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in transitions: \n",
    "    print(t[0], t[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_cap = 20\n",
    "# Replay Memory \n",
    "from collections import namedtuple, deque\n",
    "from typing import List\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, memory_capacity: int):\n",
    "        self.mem_cap = memory_capacity\n",
    "        self.reset_memory()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        self.memory = deque([], maxlen = self.mem_cap)\n",
    "    \n",
    "    def append(self, *args) -> None: \n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batchsize: int, inds: List[int] = None) -> List[Transition]:\n",
    "        if inds is None: \n",
    "            inds = np.random.choice(len(self.memory), batchsize)\n",
    "        batch = [self.memory[ind] for ind in inds]\n",
    "        return batch\n",
    "    def clear(self):\n",
    "        self.reset_memory()\n",
    "        \n",
    "        \n",
    "memory = ReplayMemory(memory_capacity = mem_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "model_info_test = copy.deepcopy(model_info)\n",
    "model_info_test[\"memory_capacity\"] = mem_cap\n",
    "model_info_test[\"batch_size\"] = 4\n",
    "print(model_info_test)\n",
    "net = build_fc_dqnn(model_info_test[\"state_dim\"], model_info_test[\"action_dim\"], model_info_test)\n",
    "model = TDWrapper(net, model_info_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in transitions: \n",
    "    model.cache(*t)\n",
    "    state, next_state, action, reward, done = t\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=\"cuda\").unsqueeze(0)\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float32, device=\"cuda\").unsqueeze(0)\n",
    "    action = torch.tensor([[action]], device=\"cuda\")\n",
    "\n",
    "    reward = torch.tensor([[reward]], device=\"cuda\")\n",
    "    done = torch.tensor([[done]], device=\"cuda\")\n",
    "    # Store the transition in memory\n",
    "    memory.append(state, action, next_state, reward, done)\n",
    "    \n",
    "print(\"======= Model Memory ===========\")\n",
    "print(len(model.memory))\n",
    "print(\"======== Memory ===========\")\n",
    "print(len(memory.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========= Model Batch =========\")\n",
    "batch_inds, *model_batch = model.recall()\n",
    "print(batch_inds)\n",
    "for x in model_batch:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a98ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========= Memory Batch ==============\")\n",
    "memory_trans = memory.sample(model_info_test[\"batch_size\"], inds = batch_inds)\n",
    "memory_batch = Transition(*zip(*memory_trans))\n",
    "\n",
    "print(memory_batch.action)\n",
    "state_batch = torch.cat(memory_batch.state)\n",
    "next_state_batch = torch.cat(memory_batch.next_state)\n",
    "action_batch = torch.cat(memory_batch.action)\n",
    "reward_batch = torch.cat(memory_batch.reward)\n",
    "done_batch = torch.cat(memory_batch.done)\n",
    "memory_batch = [state_batch, next_state_batch, action_batch, reward_batch, done_batch]\n",
    "for x in memory_batch:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582174ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal = True\n",
    "for model_batch_el, memory_batch_el in zip(model_batch, memory_batch):\n",
    "    if torch.any(model_batch_el != memory_batch_el):\n",
    "        print(model_batch_el)\n",
    "        print(memory_batch_el)\n",
    "        print(model_batch_el != memory_batch_el)\n",
    "        equal = False\n",
    "if equal: \n",
    "    print(\"Memory Equivalent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42748552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Notebook \n",
    "GAMMA = model.discount\n",
    "online_Q = model.net(state_batch, \"online\")\n",
    "online_values = online_Q.gather(1, action_batch) # Q(s, a)\n",
    "online_next_Q = model.net(next_state_batch, \"online\")\n",
    "best_action = torch.argmax(online_next_Q, axis = 1).unsqueeze(1) # a' for Q(s', -)\n",
    "bootstrap_Q = model.net(next_state_batch, \"target\")\n",
    "bootstrap_values = bootstrap_Q.gather(1, best_action) \n",
    "state_action_values = online_values\n",
    "expected_state_action_values = (bootstrap_values * (1 - done_batch.float()) * GAMMA) + reward_batch\n",
    "td_est = state_action_values\n",
    "td_target = expected_state_action_values\n",
    "criterion = nn.SmoothL1Loss()\n",
    "loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "print(f\"{td_est}\")\n",
    "print(f\"{td_target}\")\n",
    "print(f\"{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b646b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model TDs\n",
    "td_est_model = model.td_estimate(state_batch, action_batch)\n",
    "td_target_model = model.td_target(reward_batch, next_state_batch, done_batch)\n",
    "loss_model = model.loss_fn(td_est, td_target)\n",
    "print(f\"{td_est_model}\")\n",
    "print(f\"{td_target_model}\")\n",
    "print(f\"{loss_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b79b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (td_est, td_est_model),\n",
    "    (td_target, td_target_model),\n",
    "    (loss, loss_model)\n",
    "]\n",
    "equal = True\n",
    "for nb, m in pairs:\n",
    "    if torch.any(nb != m):\n",
    "        print(nb)\n",
    "        print(m)\n",
    "        print(nb != m)\n",
    "        equal = False\n",
    "if equal is True: \n",
    "    print(\"Values Equivalent\")\n",
    "    \n",
    "# Check to see if syncing is incorrect. -- it was not. \n",
    "# Check if target network is being updated in optimize step "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLVenv",
   "language": "python",
   "name": "rlvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
